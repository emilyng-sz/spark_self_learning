{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Installing Spark \n",
    "- Java is needed\n",
    "- pyspark is needed\n",
    "- create a session to use spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/emilyng/Library/Python/3.11/lib/python/site-packages/decord-0.6.0-py3.11-macosx-10.9-universal2.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyspark in /opt/homebrew/lib/python3.11/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j in /opt/homebrew/lib/python3.11/site-packages (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldnâ€™t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/qs/szbbnwjs3xqfhs8mjs1pb1xr0000gn/T/ipykernel_39372/3789082066.py\", line 8, in <module>\n",
      "    .getOrCreate()\n",
      "     ^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/session.py\", line 497, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/context.py\", line 515, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/context.py\", line 201, in __init__\n",
      "    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/context.py\", line 436, in _ensure_initialized\n",
      "    SparkContext._gateway = gateway or launch_gateway(conf)\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/java_gateway.py\", line 107, in launch_gateway\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/executing/executing.py\", line 333, in asttext\n",
      "    self._asttext = ASTText(self.text, tree=self.tree, filename=self.filename)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/asttokens/asttokens.py\", line 305, in __init__\n",
      "    super(ASTText, self).__init__(source_text, filename)\n",
      "  File \"/Users/emilyng/Library/Python/3.11/lib/python/site-packages/asttokens/asttokens.py\", line 47, in __init__\n",
      "    source_text = six.ensure_text(source_text)\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'six' has no attribute 'ensure_text'\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initiating the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyLocalSparkSession\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Checking the Spark session\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls # file name e.g. /databricks-datasets/learning-spark-v2/flights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs head # e.g. /databricks-datasets/learning-spark-v2/flights/summary-data/csv/2010-summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating objects\n",
    "\n",
    "df = spark.range(2, 1000, 2) # start, end, step\n",
    "# creates a column of numbers with column name \"id\"\n",
    "\n",
    "# df.withColumn(<column name>, <column expression>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join, show, groupby, count, orderBy etc \n",
    "df1.join(df2, df1.id == df2.id).show(10)\n",
    "# join is default inner join\n",
    "# other options: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "df.select(\"column1\", \"column2\").where(col(\"column1\").isNotNull() | col(\"col2\") == 123).distinct().orderBy(\"count\", ascending = False).show(10)\n",
    "\n",
    "# aggregating all \n",
    "df.select(sum(\"column1\"), avg(\"column2\"), max(\"column3\")).show()\n",
    "\n",
    "# use year() on (SQL Spark) for timestamp columns\n",
    "\n",
    "# drop\n",
    "df.drop(\"column1\", \"column2\").show()\n",
    "\n",
    "# rename\n",
    "df.withColumnRenamed(\"column1\", \"new_column1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL: multiple column creation and subsequent dropping of columns\n",
    "df.withColumn(\"new_column\", to_timestamp(col(\"old_col\"), \"dd/MM/yyyy\")).drop(\"old_col\")\\\n",
    "    .withColumn(\"new_col2\", col(\"new_column\") - col(\"col3\")).drop(\"old_col2\")\\\n",
    "    .withColumn(\"new_col4\", translate(col(\"col4\"), \"$\", \"\").cast(\"double\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using SQL expressions\n",
    "df = (df\n",
    "          .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "          .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "\n",
    "foo2 = foo.withColumn(\"status\", expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping rows where column subset is null\n",
    "noNullsDF = df.na.drop(subset=[\"host_is_superhost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RDD codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "df.withColumn(\"partition_id\", spark_partition_id())\n",
    "# to see which rows belong to which partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "df.cache() # to cache the dataframe in memory\n",
    "df.persist(StorageLevel.DISK_ONLY) # to cache the dataframe in memory\n",
    "\n",
    "# note that Spark will only save NEW querys. If diff variable same query plan, spark will not save new one \n",
    "# How does it work for random arrays?? can try\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "### N.B. Spark is lazy, so it will not execute until an action is called\n",
    "df.cahce().count() # in order to actually cache the dataframe!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and saving views in Spark\n",
    "\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"CACHE TABLE dfTable\")\n",
    "\n",
    "spark.sql(\"SELECT count(*) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on train documents and print columns of interest.\n",
    "pred_train = model.transform(training)\n",
    "pred_train.drop('rawPrediction').show() #.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "pred_test = model.transform(test) # test is the test data\n",
    "predictionAndLabels = pred_test.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake (add this from Demo 4 later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Saving/Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema creation:\n",
    "schema = StructType([StructField(\"name\", StringType(), True), \n",
    "                     StructField(\"age\", IntegerType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set schema when reading file\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"/tmp/fireServiceParquet/\")\n",
    "# saves it in dbfs of Databricks %fs ls /tmp/fireServiceParquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading parquet data\n",
    "df = spark.read.format(\"parquet\").load(\"/tmp/fireServiceParquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv\n",
    "airports = spark.read.options(header=\"true\", inferSchema=\"true\", sep=\"\\t\").csv(airports_path)\n",
    "airports.createOrReplaceTempView(\"airports_na\")\n",
    "rawDF = spark.read.csv(filePath, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
